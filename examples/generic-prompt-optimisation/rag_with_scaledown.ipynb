{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System with ScaleDown Prompt Optimization\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system using ScaleDown's prompt optimization features to improve response quality and reduce hallucinations.\n",
    "\n",
    "## Features Covered:\n",
    "- Document embedding and retrieval\n",
    "- RAG prompt optimization with uncertainty quantification\n",
    "- Chain-of-verification for fact-checking\n",
    "- Expert persona optimization for domain-specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scaledown chromadb sentence-transformers openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "\n",
    "# Import ScaleDown\n",
    "from scaledown import ScaleDown, optimize_prompt, parse_optimizers\n",
    "from scaledown.tools import tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API keys (set these as environment variables)\n",
    "os.environ['OPENAI_API_KEY'] = 'your-openai-api-key'\n",
    "\n",
    "# Initialize ScaleDown with optimization features\n",
    "sd = ScaleDown(enable_optimization_styles=True)\n",
    "sd.select_model('gpt-4')\n",
    "\n",
    "# Initialize embedding model and vector database\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(\"knowledge_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Ingestion and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base documents\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"text\": \"Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines. Machine Learning is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed.\",\n",
    "        \"metadata\": {\"topic\": \"AI_basics\", \"source\": \"tech_overview\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\", \n",
    "        \"text\": \"Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers. It has revolutionized fields like computer vision, natural language processing, and speech recognition.\",\n",
    "        \"metadata\": {\"topic\": \"deep_learning\", \"source\": \"tech_overview\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"text\": \"Natural Language Processing (NLP) enables computers to understand, interpret, and generate human language. Modern NLP uses transformer architectures like BERT and GPT for various tasks.\",\n",
    "        \"metadata\": {\"topic\": \"NLP\", \"source\": \"tech_overview\"}\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc4\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines information retrieval with text generation. It retrieves relevant documents from a knowledge base and uses them to generate more accurate and informed responses.\",\n",
    "        \"metadata\": {\"topic\": \"RAG\", \"source\": \"advanced_ai\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "def ingest_documents(documents: List[Dict]):\n",
    "    \"\"\"Embed and store documents in vector database\"\"\"\n",
    "    texts = [doc[\"text\"] for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts).tolist()\n",
    "    \n",
    "    collection.add(\n",
    "        embeddings=embeddings,\n",
    "        documents=texts,\n",
    "        metadatas=[doc[\"metadata\"] for doc in documents],\n",
    "        ids=[doc[\"id\"] for doc in documents]\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Ingested {len(documents)} documents\")\n",
    "\n",
    "# Ingest sample documents\n",
    "ingest_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query: str, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Retrieve most relevant documents for a query\"\"\"\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for i, doc_text in enumerate(results['documents'][0]):\n",
    "        retrieved_docs.append({\n",
    "            \"text\": doc_text,\n",
    "            \"metadata\": results['metadatas'][0][i],\n",
    "            \"distance\": results['distances'][0][i]\n",
    "        })\n",
    "    \n",
    "    return retrieved_docs\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"What is machine learning?\"\n",
    "retrieved = retrieve_documents(test_query)\n",
    "\n",
    "print(f\"Query: {test_query}\")\n",
    "print(\"\\nRetrieved documents:\")\n",
    "for i, doc in enumerate(retrieved):\n",
    "    print(f\"{i+1}. {doc['text'][:100]}... (distance: {doc['distance']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rag(query: str, top_k: int = 3) -> str:\n",
    "    \"\"\"Basic RAG without optimization\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query, top_k)\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Basic prompt template\n",
    "    prompt = f\"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please answer the question based on the provided context.\"\"\"\n",
    "    \n",
    "    # Use ScaleDown's tools API\n",
    "    result = tools(llm='gpt-4', optimiser='none')\n",
    "    llm_provider = result['llm_provider']\n",
    "    \n",
    "    response = llm_provider.call_llm(prompt, max_tokens=300)\n",
    "    return response\n",
    "\n",
    "# Test basic RAG\n",
    "query = \"How does deep learning relate to artificial intelligence?\"\n",
    "basic_response = basic_rag(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nBasic RAG Response:\\n{basic_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimized RAG with ScaleDown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_rag(query: str, top_k: int = 3, optimization_style: str = \"verified_expert\") -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced RAG with ScaleDown optimization\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retrieve_documents(query, top_k)\n",
    "    \n",
    "    # Create enhanced context with metadata\n",
    "    context_parts = []\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        source = doc['metadata'].get('source', 'unknown')\n",
    "        topic = doc['metadata'].get('topic', 'general')\n",
    "        context_parts.append(f\"Source {i+1} (Topic: {topic}, Source: {source}):\\n{doc['text']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Enhanced prompt template\n",
    "    base_prompt = f\"\"\"You are answering a question using provided context from a knowledge base.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Please provide a comprehensive answer based on the context. If the context doesn't contain sufficient information, clearly state what information is missing.\"\"\"\n",
    "    \n",
    "    # Apply ScaleDown optimization\n",
    "    if optimization_style == \"custom\":\n",
    "        # Custom optimization pipeline\n",
    "        optimizers = parse_optimizers('expert_persona,cot,uncertainty,cove')\n",
    "        optimized_prompt = optimize_prompt(base_prompt, optimizers)\n",
    "    else:\n",
    "        # Use pre-built optimization style\n",
    "        styles = sd.get_optimization_styles()\n",
    "        if optimization_style in [style['name'] for style in styles]:\n",
    "            result = sd.apply_optimization_style(base_prompt, optimization_style)\n",
    "            optimized_prompt = result['optimized_prompt']\n",
    "        else:\n",
    "            optimized_prompt = base_prompt\n",
    "    \n",
    "    # Generate response with optimized prompt\n",
    "    response_result = sd.optimize_and_call_llm(\n",
    "        question=optimized_prompt,\n",
    "        optimizers=[],  # Already optimized\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"base_prompt\": base_prompt,\n",
    "        \"optimized_prompt\": optimized_prompt,\n",
    "        \"response\": response_result['llm_response'],\n",
    "        \"optimization_metrics\": response_result.get('optimization_metrics', {})\n",
    "    }\n",
    "\n",
    "# Test optimized RAG with different styles\n",
    "query = \"How does deep learning relate to artificial intelligence?\"\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"OPTIMIZED RAG COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test with verified expert style\n",
    "result1 = optimized_rag(query, optimization_style=\"verified_expert\")\n",
    "print(f\"\\n📚 VERIFIED EXPERT STYLE:\")\n",
    "print(f\"Response: {result1['response']}\")\n",
    "\n",
    "# Test with custom optimization pipeline\n",
    "result2 = optimized_rag(query, optimization_style=\"custom\")\n",
    "print(f\"\\n🔧 CUSTOM OPTIMIZATION PIPELINE:\")\n",
    "print(f\"Response: {result2['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG with Uncertainty and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_rag_with_verification(query: str, confidence_threshold: float = 0.8) -> Dict[str, Any]:\n",
    "    \"\"\"Advanced RAG with uncertainty quantification and fact verification\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve documents\n",
    "    retrieved_docs = retrieve_documents(query, top_k=3)\n",
    "    context = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Step 2: Generate initial response with uncertainty quantification\n",
    "    uncertainty_prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please answer the question and provide a confidence score (0-1) for your answer.\"\"\"\n",
    "    \n",
    "    uncertainty_optimizers = parse_optimizers('expert_persona,uncertainty')\n",
    "    optimized_uncertainty_prompt = optimize_prompt(uncertainty_prompt, uncertainty_optimizers)\n",
    "    \n",
    "    initial_result = sd.optimize_and_call_llm(\n",
    "        question=optimized_uncertainty_prompt,\n",
    "        optimizers=[],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    initial_response = initial_result['llm_response']\n",
    "    \n",
    "    # Step 3: If confidence is low, apply chain-of-verification\n",
    "    verification_prompt = f\"\"\"Original Question: {query}\n",
    "Context: {context}\n",
    "Initial Answer: {initial_response}\n",
    "\n",
    "Please verify the accuracy of the initial answer by checking it against the provided context. Identify any potential errors or unsupported claims.\"\"\"\n",
    "    \n",
    "    verification_optimizers = parse_optimizers('expert_persona,cove')\n",
    "    optimized_verification_prompt = optimize_prompt(verification_prompt, verification_optimizers)\n",
    "    \n",
    "    verification_result = sd.optimize_and_call_llm(\n",
    "        question=optimized_verification_prompt,\n",
    "        optimizers=[],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    \n",
    "    # Step 4: Generate final verified response\n",
    "    final_prompt = f\"\"\"Based on the verification analysis, provide a final, accurate answer to: {query}\n",
    "    \n",
    "Context: {context}\n",
    "Initial Answer: {initial_response}\n",
    "Verification Analysis: {verification_result['llm_response']}\n",
    "\n",
    "Please provide the most accurate final answer.\"\"\"\n",
    "    \n",
    "    final_optimizers = parse_optimizers('expert_persona,cot')\n",
    "    optimized_final_prompt = optimize_prompt(final_prompt, final_optimizers)\n",
    "    \n",
    "    final_result = sd.optimize_and_call_llm(\n",
    "        question=optimized_final_prompt,\n",
    "        optimizers=[],\n",
    "        max_tokens=400\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"initial_response\": initial_response,\n",
    "        \"verification_analysis\": verification_result['llm_response'],\n",
    "        \"final_response\": final_result['llm_response'],\n",
    "        \"processing_steps\": [\n",
    "            \"Document Retrieval\",\n",
    "            \"Initial Response with Uncertainty\",\n",
    "            \"Chain-of-Verification\",\n",
    "            \"Final Verified Response\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Test advanced RAG\n",
    "complex_query = \"What are the key differences between machine learning and deep learning, and how do they both relate to NLP?\"\n",
    "advanced_result = advanced_rag_with_verification(complex_query)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ADVANCED RAG WITH VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuery: {complex_query}\")\n",
    "print(f\"\\n🔍 Initial Response:\\n{advanced_result['initial_response']}\")\n",
    "print(f\"\\n✅ Verification Analysis:\\n{advanced_result['verification_analysis']}\")\n",
    "print(f\"\\n🎯 Final Verified Response:\\n{advanced_result['final_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. RAG Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rag_approaches(queries: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Compare different RAG approaches\"\"\"\n",
    "    results = {\n",
    "        \"queries\": queries,\n",
    "        \"basic_rag\": [],\n",
    "        \"optimized_rag\": [],\n",
    "        \"advanced_rag\": []\n",
    "    }\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nProcessing: {query}\")\n",
    "        \n",
    "        # Basic RAG\n",
    "        basic_resp = basic_rag(query)\n",
    "        results[\"basic_rag\"].append(basic_resp)\n",
    "        \n",
    "        # Optimized RAG\n",
    "        opt_result = optimized_rag(query, optimization_style=\"verified_expert\")\n",
    "        results[\"optimized_rag\"].append(opt_result['response'])\n",
    "        \n",
    "        # Advanced RAG\n",
    "        adv_result = advanced_rag_with_verification(query)\n",
    "        results[\"advanced_rag\"].append(adv_result['final_response'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is the relationship between AI and machine learning?\",\n",
    "    \"How does RAG improve AI responses?\",\n",
    "    \"What are the applications of deep learning in NLP?\"\n",
    "]\n",
    "\n",
    "# Run comparison (uncomment to execute)\n",
    "# comparison_results = compare_rag_approaches(test_queries)\n",
    "\n",
    "print(\"\\n📊 RAG COMPARISON FRAMEWORK READY\")\n",
    "print(\"Uncomment the comparison_results line to run full comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_response(query: str, response: str, retrieved_docs: List[Dict]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate RAG response quality\"\"\"\n",
    "    \n",
    "    # Evaluation prompt using ScaleDown optimization\n",
    "    eval_prompt = f\"\"\"Evaluate the following RAG system response on a scale of 1-10 for each criterion:\n",
    "\n",
    "Query: {query}\n",
    "Response: {response}\n",
    "Retrieved Context: {[doc['text'][:100] + '...' for doc in retrieved_docs]}\n",
    "\n",
    "Criteria:\n",
    "1. Accuracy: How factually correct is the response?\n",
    "2. Relevance: How well does it answer the specific question?\n",
    "3. Completeness: Does it provide comprehensive coverage?\n",
    "4. Coherence: Is the response well-structured and logical?\n",
    "5. Context Usage: How well does it utilize the retrieved context?\n",
    "\n",
    "Provide scores as: Accuracy: X, Relevance: Y, Completeness: Z, Coherence: A, Context Usage: B\"\"\"\n",
    "    \n",
    "    # Use expert persona for evaluation\n",
    "    eval_optimizers = parse_optimizers('expert_persona')\n",
    "    optimized_eval_prompt = optimize_prompt(eval_prompt, eval_optimizers)\n",
    "    \n",
    "    eval_result = sd.optimize_and_call_llm(\n",
    "        question=optimized_eval_prompt,\n",
    "        optimizers=[],\n",
    "        max_tokens=200\n",
    "    )\n",
    "    \n",
    "    # Parse evaluation scores (simplified parsing)\n",
    "    eval_text = eval_result['llm_response']\n",
    "    \n",
    "    return {\n",
    "        \"evaluation_text\": eval_text,\n",
    "        \"retrieval_count\": len(retrieved_docs),\n",
    "        \"response_length\": len(response),\n",
    "        \"avg_doc_distance\": sum(doc['distance'] for doc in retrieved_docs) / len(retrieved_docs)\n",
    "    }\n",
    "\n",
    "# Example evaluation\n",
    "sample_query = \"What is machine learning?\"\n",
    "sample_docs = retrieve_documents(sample_query)\n",
    "sample_response = \"Machine learning is a subset of AI that enables computers to learn from experience.\"\n",
    "\n",
    "evaluation = evaluate_rag_response(sample_query, sample_response, sample_docs)\n",
    "print(f\"\\n📈 EVALUATION RESULTS:\")\n",
    "print(f\"Query: {sample_query}\")\n",
    "print(f\"Evaluation: {evaluation['evaluation_text']}\")\n",
    "print(f\"Metrics: {evaluation['retrieval_count']} docs, avg distance: {evaluation['avg_doc_distance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Basic RAG Implementation**: Simple retrieval + generation\n",
    "2. **ScaleDown Optimization**: Enhanced prompts with expert persona, CoT, uncertainty\n",
    "3. **Advanced Verification**: Chain-of-verification for fact-checking\n",
    "4. **Performance Comparison**: Side-by-side evaluation of approaches\n",
    "5. **Evaluation Framework**: Metrics for response quality assessment\n",
    "\n",
    "### Key Benefits of ScaleDown in RAG:\n",
    "- **Reduced Hallucinations**: Uncertainty quantification and verification\n",
    "- **Improved Accuracy**: Expert persona and domain-specific optimization\n",
    "- **Better Reasoning**: Chain-of-thought for complex queries\n",
    "- **Flexible Optimization**: Mix and match optimizers for specific use cases\n",
    "\n",
    "### Next Steps:\n",
    "- Implement custom optimizers for your domain\n",
    "- Add more sophisticated evaluation metrics\n",
    "- Scale to larger knowledge bases\n",
    "- Integrate with production systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}